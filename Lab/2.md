## Lab 2： 主/备k-v服务
### 1 简介
mapreduce实验中错误处理是很简单的，因为worker是无状态的，状态由master维护，但是我们没有要求master的容错性。实验2向有状态的服务的容错性迈出了第一步。

### 2 实验2-5路线图
在接下来的4个实验中，你会实现若干个k-v服务，这些服务支持三个RPC：Put(key, value)、Append(key, arg)和Get(key)。服务维护了一个简单的k-v键值对的数据库，Put()替换数据库中某个key的value，Append()向key的value追加arg，Get()获得一个key当前的value。

这些实验的区别在于k-v服务的容错性和性能：

* 实验2使用主/备冗余，viewservice用于决定哪个机器是正常的。viewservice允许主/备服务在划分网段（network partition）的情况下也能正常工作。viewservice本身没有副本，因此它是个故障的单点。
* 实验3使用paxos协议对k-v服务做副本，从而实现没有单点故障，并且能够正确的处理划分网段。这种k-v服务比无副本的k-v服务响应速度慢，但是具有容错性。
* 实验4是一个分片的（sharded）的k-v数据库，每个分片使用paxos对自己的状态做副本。这个k-v服务能够在不同的分片上并行的进行Put/Get操作，这一特性适合mapreduce这类对存储系统有较高负载压力的应用。除此之外，实验4还有一个带副本的配置服务，用于告诉每个分片，它们存储的key的范围。这个服务可以改变分片中key的分配，例如响应动态变化的负载。实验4是真实世界中一个1000+服务器的设计原型。
* 实验5在实验4中增加了持久性，一个k-v服务器可以从宕机中恢复并重新加入到它的冗余组中（replica group）。

每个实验中你会做大量的设计，我们提供了整体涉及的框架代码（以及一些乏味功能的实现代码），要求你填充框架中缺失的部分并制定完整的通信协议。测试用例对你制定的通信协议、容错能力以及基本性能进行测试，对于测试中暴露的问题，需要你重新进行设计和实现。仔细的思考和计划可以帮助你减少迭代的次数。我们不提供测试用例的描述（但是提供测试用例的Go源码），在真实世界中，测试用例应该是由你自己想出来。

### 3 实验2概览
这个实验中你会实现一个k-v服务，通过主/备冗余的方式实现容错性。为了确保所有的参与者（客户端和服务器）对于谁是主机谁是备份达成一致，我们引入一个master，叫做viewservice。这个服务用于监控所有服务器存活状态，如果当前的主机或者备份宕机了，viewservice会重新选择一个服务器替代它。客户端通过viewservice获得当前的主机。所有的服务器都和viewservice交互，确保同一个时间只有最多一个可用的主机。

你的k-v服务需要允许替换故障的服务器。如果主机故障了，viewservice会把备份升级为主机。如果备份故障了或者备份升级为主机，并且有一个空闲可用的服务器，viewservice会把它作为新的备份。主机会把它的全部数据库发送到新的备份，并且把所有的Put操作也发送给备份，确保备份上的k-v数据库和主机上保持一致。

主机必须向从机发送Get和Put操作，并且必须等待从机应答以后才能把结果返回给客户端。这样是为了避免两个服务器都作为主机（分裂的大脑）。例如：S1是主机，S2是从机。viewservice错误的判断S1宕机了，因此把S2提升为主机。此时一个客户端还认为主机是S1并相它发送了请求，S1会把请求转发给S2（译者注：因为S2是从机，主机会把请求转发给从机），因为S2已经被viewservice提升为主机了，因此它向S1返回错误。S1会向客户端返回错误，告诉客户端S1可能不是主机了（因为S2拒绝了请求），客户端会向viewservice请求新的主机（即S2），然后向它发送请求。

故障的k-v服务器可能重启，但是重启后不会拷贝备份数据（but it will do so without a copy of the replicated data）。数据保存在你的k-v服务器内存中而不是硬盘，这样做的结果是如果没有备份并且主机宕机了，主机重启后就不能再作为主机了（译者注：因为内存中的数据丢失了）。

不同客户端之间、服务器之间以及客户端和服务器之间只能使用RPC通信。服务器的不同实例不允许共享Go的变量或者文件。

上面的设计有一定的容错性和性能局限，真实世界中很少使用：
* viewservice容易故障，因为它是单点
* 主机和备份只能逐个处理请求，限制了系统性能
* 备份服务器必须从主机拷贝完整的k-v数据库，这个时间可能很长，机器备份服务器和主机上的数据几乎是一致的（比如：因为短时间的网络连接故障丢失了几分钟内的更新操作）。
* 服务器不在磁盘上保存k-v数据库，所以主机和备份同时宕机时就无法恢复（例如：整个网站机房停电）。
* 如果有一个临时的问题阻止了主机和从机之间的通信，系统有两种解救办法：viewservice消除备份服务器，或者是（主机不断）重试。这两种方法在这个问题频发时都不能很好的解决。
* 如果主机在viewservice知道哪个是主机之前就宕机了，viewservice就没法继续了，它会一直循环（等待主机通知）并且不能主动更新主机。

我们会在后面的实验中通过优化设计和协议来处理这些局限（address these limitations），本次实验会帮助你理解后面实验需要解决的问题。

这个实验中的主/备模型没有可以参考的已经公布的原型，这个实验也没有制定一个完整的系统原型，你必须自己来完善细节。这个实验的原型和Flat Datacenter Storage有些类似（viewservice和FDS的metadata是类似的，主/备服务器和FDS的tract服务器也很相像），但是FDS对于提高性能进行了很多的考虑。这个实验和MongoDB的副本集合也有一些类似（但是MongoDB使用类paxos选举算法选主）。更多关于主/备原型的详细描述，请参考[Chain Replication](http://www.cs.cornell.edu/home/rvr/papers/osdi04.pdf)。Chain Replication比实验2有更好的性能表现，though it assumes that the view service never declares a server dead when it is merely partitioned。关于高性能的主/备设计的细节以及在各种故障下恢复系统状态，请参考Harp and Viewstamped Replication。

### 4 合作政策（略）

### 5 软件
使用git pull获取最新的实验代码，我们提供了实验2的框架代码和测试在src/viewservice和src/pbsrevice。

忽略methd Kill错误信息，测试失败是因为viewservice/server.go的RPC handler为空。

你也可以（不运行测试）单独启动程序，相关的代码为main/viewd.go、main/pbd.go和main/pbc.go，请阅读pbc.go中的注释。

```shell
$ add 6.824
$ cd ~/6.824
$ git pull
...
$ cd src/viewservice
$ go test
2012/12/28 14:51:47 method Kill has wrong number of ins: 1
First primary: --- FAIL: Test1 (1.02 seconds)
        test_test.go:13: wanted primary /var/tmp/viewserver-35913-1, got 
FAIL
exit status 1
FAIL    _/afs/athena.mit.edu/user/r/t/rtm/6.824/src/viewservice 1.041s
$
```

### 6 Part A：viewservice

![](lab2_1.png)

### 7 Part B：主/备k-v服务

### 8 提交过程（略）